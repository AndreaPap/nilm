{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"gP0c8CJkZrkh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687682237536,"user_tz":-120,"elapsed":9386,"user":{"displayName":"Andrea Papiri","userId":"03585521923946040254"}},"outputId":"5830b334-d306-4736-a3cf-5051cbc862bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_adapt\n","  Downloading pytorch_adapt-0.0.83-py3-none-any.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_adapt) (1.22.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch_adapt) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from pytorch_adapt) (0.15.2+cu118)\n","Collecting torchmetrics==0.9.3 (from pytorch_adapt)\n","  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.6/419.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-metric-learning==1.6.3 (from pytorch_adapt)\n","  Downloading pytorch_metric_learning-1.6.3-py3-none-any.whl (111 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.4/111.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning==1.6.3->pytorch_adapt) (1.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning==1.6.3->pytorch_adapt) (4.65.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics==0.9.3->pytorch_adapt) (23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_adapt) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_adapt) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_adapt) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_adapt) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_adapt) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_adapt) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch_adapt) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch_adapt) (16.0.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->pytorch_adapt) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->pytorch_adapt) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch_adapt) (2.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->pytorch_adapt) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->pytorch_adapt) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->pytorch_adapt) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->pytorch_adapt) (3.4)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning==1.6.3->pytorch_adapt) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning==1.6.3->pytorch_adapt) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning==1.6.3->pytorch_adapt) (3.1.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch_adapt) (1.3.0)\n","Installing collected packages: torchmetrics, pytorch-metric-learning, pytorch_adapt\n","Successfully installed pytorch-metric-learning-1.6.3 pytorch_adapt-0.0.83 torchmetrics-0.9.3\n"]}],"source":["pip install pytorch_adapt"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpmgcAZutl71","executionInfo":{"status":"ok","timestamp":1687687784376,"user_tz":-120,"elapsed":1624250,"user":{"displayName":"Andrea Papiri","userId":"03585521923946040254"}},"outputId":"b9dfb17c-4559-47c8-8292-c78f351e0ac7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","fridge 0.001\n","fixed mode\n","working on: cuda:0\n","##########\tITERATION   0/1500\t##########\n","training loss: 1.921\n","testing loss: 1.902\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   1/1500\t##########\n","training loss: 1.014\n","testing loss: 0.812\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   2/1500\t##########\n","training loss: 1.185\n","testing loss: 0.775\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   3/1500\t##########\n","training loss: 1.055\n","testing loss: 0.722\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   4/1500\t##########\n","training loss: 1.021\n","testing loss: 0.743\n","##################################################\n","##########\tITERATION   5/1500\t##########\n","training loss: 0.997\n","testing loss: 0.752\n","##################################################\n","##########\tITERATION   6/1500\t##########\n","training loss: 0.961\n","testing loss: 0.672\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   7/1500\t##########\n","training loss: 1.001\n","testing loss: 0.754\n","##################################################\n","##########\tITERATION   8/1500\t##########\n","training loss: 0.944\n","testing loss: 0.676\n","##################################################\n","##########\tITERATION   9/1500\t##########\n","training loss: 0.953\n","testing loss: 0.617\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   10/1500\t##########\n","training loss: 0.915\n","testing loss: 0.633\n","##################################################\n","##########\tITERATION   11/1500\t##########\n","training loss: 0.923\n","testing loss: 0.614\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   12/1500\t##########\n","training loss: 0.901\n","testing loss: 0.625\n","##################################################\n","##########\tITERATION   13/1500\t##########\n","training loss: 0.88\n","testing loss: 0.685\n","##################################################\n","##########\tITERATION   14/1500\t##########\n","training loss: 0.852\n","testing loss: 0.622\n","##################################################\n","##########\tITERATION   15/1500\t##########\n","training loss: 0.886\n","testing loss: 0.556\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   16/1500\t##########\n","training loss: 0.776\n","testing loss: 0.609\n","##################################################\n","##########\tITERATION   17/1500\t##########\n","training loss: 0.828\n","testing loss: 0.564\n","##################################################\n","##########\tITERATION   18/1500\t##########\n","training loss: 0.894\n","testing loss: 0.607\n","##################################################\n","##########\tITERATION   19/1500\t##########\n","training loss: 0.873\n","testing loss: 0.566\n","##################################################\n","##########\tITERATION   20/1500\t##########\n","training loss: 0.853\n","testing loss: 0.579\n","##################################################\n","##########\tITERATION   21/1500\t##########\n","training loss: 0.893\n","testing loss: 0.6\n","##################################################\n","##########\tITERATION   22/1500\t##########\n","training loss: 0.86\n","testing loss: 0.564\n","##################################################\n","##########\tITERATION   23/1500\t##########\n","training loss: 0.745\n","testing loss: 0.537\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   24/1500\t##########\n","training loss: 0.78\n","testing loss: 0.606\n","##################################################\n","##########\tITERATION   25/1500\t##########\n","training loss: 0.811\n","testing loss: 0.579\n","##################################################\n","##########\tITERATION   26/1500\t##########\n","training loss: 0.926\n","testing loss: 0.62\n","##################################################\n","##########\tITERATION   27/1500\t##########\n","training loss: 0.826\n","testing loss: 0.546\n","##################################################\n","##########\tITERATION   28/1500\t##########\n","training loss: 0.846\n","testing loss: 0.519\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   29/1500\t##########\n","training loss: 0.803\n","testing loss: 0.536\n","##################################################\n","##########\tITERATION   30/1500\t##########\n","training loss: 0.795\n","testing loss: 0.55\n","##################################################\n","##########\tITERATION   31/1500\t##########\n","training loss: 0.829\n","testing loss: 0.611\n","##################################################\n","##########\tITERATION   32/1500\t##########\n","training loss: 0.819\n","testing loss: 0.585\n","##################################################\n","##########\tITERATION   33/1500\t##########\n","training loss: 0.675\n","testing loss: 0.49\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   34/1500\t##########\n","training loss: 0.727\n","testing loss: 0.572\n","##################################################\n","##########\tITERATION   35/1500\t##########\n","training loss: 0.763\n","testing loss: 0.584\n","##################################################\n","##########\tITERATION   36/1500\t##########\n","training loss: 0.767\n","testing loss: 0.591\n","##################################################\n","##########\tITERATION   37/1500\t##########\n","training loss: 0.815\n","testing loss: 0.551\n","##################################################\n","##########\tITERATION   38/1500\t##########\n","training loss: 0.765\n","testing loss: 0.519\n","##################################################\n","##########\tITERATION   39/1500\t##########\n","training loss: 0.824\n","testing loss: 0.557\n","##################################################\n","##########\tITERATION   40/1500\t##########\n","training loss: 0.831\n","testing loss: 0.538\n","##################################################\n","##########\tITERATION   41/1500\t##########\n","training loss: 0.765\n","testing loss: 0.617\n","##################################################\n","##########\tITERATION   42/1500\t##########\n","training loss: 0.709\n","testing loss: 0.584\n","##################################################\n","##########\tITERATION   43/1500\t##########\n","training loss: 0.701\n","testing loss: 0.504\n","##################################################\n","##########\tITERATION   44/1500\t##########\n","training loss: 0.746\n","testing loss: 0.511\n","##################################################\n","##########\tITERATION   45/1500\t##########\n","training loss: 0.789\n","testing loss: 0.586\n","##################################################\n","##########\tITERATION   46/1500\t##########\n","training loss: 0.703\n","testing loss: 0.534\n","##################################################\n","##########\tITERATION   47/1500\t##########\n","training loss: 0.76\n","testing loss: 0.52\n","##################################################\n","##########\tITERATION   48/1500\t##########\n","training loss: 0.747\n","testing loss: 0.621\n","##################################################\n","##########\tITERATION   49/1500\t##########\n","training loss: 0.726\n","testing loss: 0.514\n","##################################################\n","##########\tITERATION   50/1500\t##########\n","training loss: 0.752\n","testing loss: 0.54\n","##################################################\n","##########\tITERATION   51/1500\t##########\n","training loss: 0.764\n","testing loss: 0.522\n","##################################################\n","##########\tITERATION   52/1500\t##########\n","training loss: 0.776\n","testing loss: 0.576\n","##################################################\n","##########\tITERATION   53/1500\t##########\n","training loss: 0.748\n","testing loss: 0.588\n","##################################################\n","##########\tITERATION   54/1500\t##########\n","training loss: 0.745\n","testing loss: 0.541\n","##################################################\n","##########\tITERATION   55/1500\t##########\n","training loss: 0.738\n","testing loss: 0.533\n","##################################################\n","##########\tITERATION   56/1500\t##########\n","training loss: 0.694\n","testing loss: 0.466\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   57/1500\t##########\n","training loss: 0.783\n","testing loss: 0.546\n","##################################################\n","##########\tITERATION   58/1500\t##########\n","training loss: 0.679\n","testing loss: 0.485\n","##################################################\n","##########\tITERATION   59/1500\t##########\n","training loss: 0.812\n","testing loss: 0.594\n","##################################################\n","##########\tITERATION   60/1500\t##########\n","training loss: 0.695\n","testing loss: 0.519\n","##################################################\n","##########\tITERATION   61/1500\t##########\n","training loss: 0.675\n","testing loss: 0.554\n","##################################################\n","##########\tITERATION   62/1500\t##########\n","training loss: 0.673\n","testing loss: 0.459\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   63/1500\t##########\n","training loss: 0.669\n","testing loss: 0.522\n","##################################################\n","##########\tITERATION   64/1500\t##########\n","training loss: 0.678\n","testing loss: 0.547\n","##################################################\n","##########\tITERATION   65/1500\t##########\n","training loss: 0.756\n","testing loss: 0.535\n","##################################################\n","##########\tITERATION   66/1500\t##########\n","training loss: 0.686\n","testing loss: 0.521\n","##################################################\n","##########\tITERATION   67/1500\t##########\n","training loss: 0.662\n","testing loss: 0.529\n","##################################################\n","##########\tITERATION   68/1500\t##########\n","training loss: 0.766\n","testing loss: 0.491\n","##################################################\n","##########\tITERATION   69/1500\t##########\n","training loss: 0.73\n","testing loss: 0.528\n","##################################################\n","##########\tITERATION   70/1500\t##########\n","training loss: 0.667\n","testing loss: 0.511\n","##################################################\n","##########\tITERATION   71/1500\t##########\n","training loss: 0.709\n","testing loss: 0.563\n","##################################################\n","##########\tITERATION   72/1500\t##########\n","training loss: 0.615\n","testing loss: 0.554\n","##################################################\n","##########\tITERATION   73/1500\t##########\n","training loss: 0.712\n","testing loss: 0.531\n","##################################################\n","##########\tITERATION   74/1500\t##########\n","training loss: 0.679\n","testing loss: 0.487\n","##################################################\n","##########\tITERATION   75/1500\t##########\n","training loss: 0.679\n","testing loss: 0.491\n","##################################################\n","##########\tITERATION   76/1500\t##########\n","training loss: 0.755\n","testing loss: 0.498\n","##################################################\n","##########\tITERATION   77/1500\t##########\n","training loss: 0.719\n","testing loss: 0.513\n","##################################################\n","##########\tITERATION   78/1500\t##########\n","training loss: 0.692\n","testing loss: 0.549\n","##################################################\n","##########\tITERATION   79/1500\t##########\n","training loss: 0.713\n","testing loss: 0.529\n","##################################################\n","##########\tITERATION   80/1500\t##########\n","training loss: 0.71\n","testing loss: 0.54\n","##################################################\n","##########\tITERATION   81/1500\t##########\n","training loss: 0.692\n","testing loss: 0.484\n","##################################################\n","##########\tITERATION   82/1500\t##########\n","training loss: 0.722\n","testing loss: 0.522\n","##################################################\n","##########\tITERATION   83/1500\t##########\n","training loss: 0.752\n","testing loss: 0.507\n","##################################################\n","##########\tITERATION   84/1500\t##########\n","training loss: 0.722\n","testing loss: 0.52\n","##################################################\n","##########\tITERATION   85/1500\t##########\n","training loss: 0.683\n","testing loss: 0.539\n","##################################################\n","##########\tITERATION   86/1500\t##########\n","training loss: 0.741\n","testing loss: 0.506\n","##################################################\n","##########\tITERATION   87/1500\t##########\n","training loss: 0.795\n","testing loss: 0.533\n","##################################################\n","##########\tITERATION   88/1500\t##########\n","training loss: 0.708\n","testing loss: 0.475\n","##################################################\n","##########\tITERATION   89/1500\t##########\n","training loss: 0.68\n","testing loss: 0.534\n","##################################################\n","##########\tITERATION   90/1500\t##########\n","training loss: 0.727\n","testing loss: 0.539\n","##################################################\n","##########\tITERATION   91/1500\t##########\n","training loss: 0.676\n","testing loss: 0.514\n","##################################################\n","##########\tITERATION   92/1500\t##########\n","training loss: 0.718\n","testing loss: 0.548\n","(<class 'Exception'>, Exception('Net stop to learn'), <traceback object at 0x7fce97730fc0>)\n","best testing loss: 0.459 @ 62\n","##########\tEND\t##########\n","fridge 0.001\n","fixed mode\n","working on: cuda:0\n","##########\tITERATION   0/1500\t##########\n","training loss: 0.973\n","testing loss: 0.754\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   1/1500\t##########\n","training loss: 0.956\n","testing loss: 0.791\n","##################################################\n","##########\tITERATION   2/1500\t##########\n","training loss: 0.944\n","testing loss: 0.78\n","##################################################\n","##########\tITERATION   3/1500\t##########\n","training loss: 0.926\n","testing loss: 0.651\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   4/1500\t##########\n","training loss: 0.906\n","testing loss: 0.637\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   5/1500\t##########\n","training loss: 0.94\n","testing loss: 0.631\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   6/1500\t##########\n","training loss: 0.863\n","testing loss: 0.661\n","##################################################\n","##########\tITERATION   7/1500\t##########\n","training loss: 0.835\n","testing loss: 0.694\n","##################################################\n","##########\tITERATION   8/1500\t##########\n","training loss: 0.895\n","testing loss: 0.653\n","##################################################\n","##########\tITERATION   9/1500\t##########\n","training loss: 0.84\n","testing loss: 0.603\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   10/1500\t##########\n","training loss: 0.875\n","testing loss: 0.582\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   11/1500\t##########\n","training loss: 0.863\n","testing loss: 0.552\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   12/1500\t##########\n","training loss: 0.805\n","testing loss: 0.615\n","##################################################\n","##########\tITERATION   13/1500\t##########\n","training loss: 0.846\n","testing loss: 0.662\n","##################################################\n","##########\tITERATION   14/1500\t##########\n","training loss: 0.825\n","testing loss: 0.584\n","##################################################\n","##########\tITERATION   15/1500\t##########\n","training loss: 0.835\n","testing loss: 0.574\n","##################################################\n","##########\tITERATION   16/1500\t##########\n","training loss: 0.882\n","testing loss: 0.537\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   17/1500\t##########\n","training loss: 0.754\n","testing loss: 0.566\n","##################################################\n","##########\tITERATION   18/1500\t##########\n","training loss: 0.805\n","testing loss: 0.572\n","##################################################\n","##########\tITERATION   19/1500\t##########\n","training loss: 0.803\n","testing loss: 0.553\n","##################################################\n","##########\tITERATION   20/1500\t##########\n","training loss: 0.774\n","testing loss: 0.589\n","##################################################\n","##########\tITERATION   21/1500\t##########\n","training loss: 0.762\n","testing loss: 0.546\n","##################################################\n","##########\tITERATION   22/1500\t##########\n","training loss: 0.749\n","testing loss: 0.531\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   23/1500\t##########\n","training loss: 0.808\n","testing loss: 0.587\n","##################################################\n","##########\tITERATION   24/1500\t##########\n","training loss: 0.718\n","testing loss: 0.56\n","##################################################\n","##########\tITERATION   25/1500\t##########\n","training loss: 0.814\n","testing loss: 0.559\n","##################################################\n","##########\tITERATION   26/1500\t##########\n","training loss: 0.757\n","testing loss: 0.554\n","##################################################\n","##########\tITERATION   27/1500\t##########\n","training loss: 0.764\n","testing loss: 0.534\n","##################################################\n","##########\tITERATION   28/1500\t##########\n","training loss: 0.674\n","testing loss: 0.555\n","##################################################\n","##########\tITERATION   29/1500\t##########\n","training loss: 0.72\n","testing loss: 0.557\n","##################################################\n","##########\tITERATION   30/1500\t##########\n","training loss: 0.75\n","testing loss: 0.554\n","##################################################\n","##########\tITERATION   31/1500\t##########\n","training loss: 0.764\n","testing loss: 0.546\n","##################################################\n","##########\tITERATION   32/1500\t##########\n","training loss: 0.756\n","testing loss: 0.526\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   33/1500\t##########\n","training loss: 0.734\n","testing loss: 0.547\n","##################################################\n","##########\tITERATION   34/1500\t##########\n","training loss: 0.747\n","testing loss: 0.537\n","##################################################\n","##########\tITERATION   35/1500\t##########\n","training loss: 0.697\n","testing loss: 0.488\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   36/1500\t##########\n","training loss: 0.745\n","testing loss: 0.512\n","##################################################\n","##########\tITERATION   37/1500\t##########\n","training loss: 0.809\n","testing loss: 0.484\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   38/1500\t##########\n","training loss: 0.766\n","testing loss: 0.504\n","##################################################\n","##########\tITERATION   39/1500\t##########\n","training loss: 0.739\n","testing loss: 0.587\n","##################################################\n","##########\tITERATION   40/1500\t##########\n","training loss: 0.737\n","testing loss: 0.57\n","##################################################\n","##########\tITERATION   41/1500\t##########\n","training loss: 0.74\n","testing loss: 0.52\n","##################################################\n","##########\tITERATION   42/1500\t##########\n","training loss: 0.726\n","testing loss: 0.502\n","##################################################\n","##########\tITERATION   43/1500\t##########\n","training loss: 0.683\n","testing loss: 0.499\n","##################################################\n","##########\tITERATION   44/1500\t##########\n","training loss: 0.78\n","testing loss: 0.528\n","##################################################\n","##########\tITERATION   45/1500\t##########\n","training loss: 0.743\n","testing loss: 0.577\n","##################################################\n","##########\tITERATION   46/1500\t##########\n","training loss: 0.793\n","testing loss: 0.58\n","##################################################\n","##########\tITERATION   47/1500\t##########\n","training loss: 0.798\n","testing loss: 0.56\n","##################################################\n","##########\tITERATION   48/1500\t##########\n","training loss: 0.734\n","testing loss: 0.497\n","##################################################\n","##########\tITERATION   49/1500\t##########\n","training loss: 0.706\n","testing loss: 0.544\n","##################################################\n","##########\tITERATION   50/1500\t##########\n","training loss: 0.726\n","testing loss: 0.554\n","##################################################\n","##########\tITERATION   51/1500\t##########\n","training loss: 0.663\n","testing loss: 0.484\n","##################################################\n","##########\tITERATION   52/1500\t##########\n","training loss: 0.823\n","testing loss: 0.494\n","##################################################\n","##########\tITERATION   53/1500\t##########\n","training loss: 0.732\n","testing loss: 0.492\n","##################################################\n","##########\tITERATION   54/1500\t##########\n","training loss: 0.735\n","testing loss: 0.609\n","##################################################\n","##########\tITERATION   55/1500\t##########\n","training loss: 0.729\n","testing loss: 0.572\n","##################################################\n","##########\tITERATION   56/1500\t##########\n","training loss: 0.76\n","testing loss: 0.484\n","##################################################\n","##########\tITERATION   57/1500\t##########\n","training loss: 0.667\n","testing loss: 0.505\n","##################################################\n","##########\tITERATION   58/1500\t##########\n","training loss: 0.769\n","testing loss: 0.505\n","##################################################\n","##########\tITERATION   59/1500\t##########\n","training loss: 0.685\n","testing loss: 0.493\n","##################################################\n","##########\tITERATION   60/1500\t##########\n","training loss: 0.705\n","testing loss: 0.49\n","##################################################\n","##########\tITERATION   61/1500\t##########\n","training loss: 0.782\n","testing loss: 0.567\n","##################################################\n","##########\tITERATION   62/1500\t##########\n","training loss: 0.743\n","testing loss: 0.566\n","##################################################\n","##########\tITERATION   63/1500\t##########\n","training loss: 0.843\n","testing loss: 0.521\n","##################################################\n","##########\tITERATION   64/1500\t##########\n","training loss: 0.712\n","testing loss: 0.497\n","##################################################\n","##########\tITERATION   65/1500\t##########\n","training loss: 0.734\n","testing loss: 0.486\n","##################################################\n","##########\tITERATION   66/1500\t##########\n","training loss: 0.704\n","testing loss: 0.505\n","##################################################\n","##########\tITERATION   67/1500\t##########\n","training loss: 0.757\n","testing loss: 0.53\n","##################################################\n","##########\tITERATION   68/1500\t##########\n","training loss: 0.72\n","testing loss: 0.51\n","##################################################\n","##########\tITERATION   69/1500\t##########\n","training loss: 0.728\n","testing loss: 0.505\n","##################################################\n","##########\tITERATION   70/1500\t##########\n","training loss: 0.741\n","testing loss: 0.474\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   71/1500\t##########\n","training loss: 0.761\n","testing loss: 0.464\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   72/1500\t##########\n","training loss: 0.646\n","testing loss: 0.529\n","##################################################\n","##########\tITERATION   73/1500\t##########\n","training loss: 0.705\n","testing loss: 0.516\n","##################################################\n","##########\tITERATION   74/1500\t##########\n","training loss: 0.683\n","testing loss: 0.52\n","##################################################\n","##########\tITERATION   75/1500\t##########\n","training loss: 0.758\n","testing loss: 0.499\n","##################################################\n","##########\tITERATION   76/1500\t##########\n","training loss: 0.728\n","testing loss: 0.518\n","##################################################\n","##########\tITERATION   77/1500\t##########\n","training loss: 0.648\n","testing loss: 0.565\n","##################################################\n","##########\tITERATION   78/1500\t##########\n","training loss: 0.701\n","testing loss: 0.55\n","##################################################\n","##########\tITERATION   79/1500\t##########\n","training loss: 0.782\n","testing loss: 0.463\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   80/1500\t##########\n","training loss: 0.744\n","testing loss: 0.485\n","##################################################\n","##########\tITERATION   81/1500\t##########\n","training loss: 0.723\n","testing loss: 0.498\n","##################################################\n","##########\tITERATION   82/1500\t##########\n","training loss: 0.7\n","testing loss: 0.531\n","##################################################\n","##########\tITERATION   83/1500\t##########\n","training loss: 0.673\n","testing loss: 0.478\n","##################################################\n","##########\tITERATION   84/1500\t##########\n","training loss: 0.712\n","testing loss: 0.496\n","##################################################\n","##########\tITERATION   85/1500\t##########\n","training loss: 0.704\n","testing loss: 0.504\n","##################################################\n","##########\tITERATION   86/1500\t##########\n","training loss: 0.709\n","testing loss: 0.463\n","##################################################\n","##########\tITERATION   87/1500\t##########\n","training loss: 0.703\n","testing loss: 0.517\n","##################################################\n","##########\tITERATION   88/1500\t##########\n","training loss: 0.719\n","testing loss: 0.531\n","##################################################\n","##########\tITERATION   89/1500\t##########\n","training loss: 0.733\n","testing loss: 0.515\n","##################################################\n","##########\tITERATION   90/1500\t##########\n","training loss: 0.715\n","testing loss: 0.524\n","##################################################\n","##########\tITERATION   91/1500\t##########\n","training loss: 0.718\n","testing loss: 0.484\n","##################################################\n","##########\tITERATION   92/1500\t##########\n","training loss: 0.683\n","testing loss: 0.468\n","##################################################\n","##########\tITERATION   93/1500\t##########\n","training loss: 0.728\n","testing loss: 0.468\n","##################################################\n","##########\tITERATION   94/1500\t##########\n","training loss: 0.695\n","testing loss: 0.499\n","##################################################\n","##########\tITERATION   95/1500\t##########\n","training loss: 0.636\n","testing loss: 0.496\n","##################################################\n","##########\tITERATION   96/1500\t##########\n","training loss: 0.709\n","testing loss: 0.491\n","##################################################\n","##########\tITERATION   97/1500\t##########\n","training loss: 0.735\n","testing loss: 0.529\n","##################################################\n","##########\tITERATION   98/1500\t##########\n","training loss: 0.715\n","testing loss: 0.521\n","##################################################\n","##########\tITERATION   99/1500\t##########\n","training loss: 0.707\n","testing loss: 0.507\n","##################################################\n","##########\tITERATION   100/1500\t##########\n","training loss: 0.67\n","testing loss: 0.512\n","##################################################\n","##########\tITERATION   101/1500\t##########\n","training loss: 0.704\n","testing loss: 0.537\n","##################################################\n","##########\tITERATION   102/1500\t##########\n","training loss: 0.781\n","testing loss: 0.481\n","##################################################\n","##########\tITERATION   103/1500\t##########\n","training loss: 0.71\n","testing loss: 0.481\n","##################################################\n","##########\tITERATION   104/1500\t##########\n","training loss: 0.766\n","testing loss: 0.5\n","##################################################\n","##########\tITERATION   105/1500\t##########\n","training loss: 0.745\n","testing loss: 0.502\n","##################################################\n","##########\tITERATION   106/1500\t##########\n","training loss: 0.678\n","testing loss: 0.498\n","##################################################\n","##########\tITERATION   107/1500\t##########\n","training loss: 0.672\n","testing loss: 0.56\n","##################################################\n","##########\tITERATION   108/1500\t##########\n","training loss: 0.69\n","testing loss: 0.559\n","##################################################\n","##########\tITERATION   109/1500\t##########\n","training loss: 0.759\n","testing loss: 0.501\n","##################################################\n","##########\tITERATION   110/1500\t##########\n","training loss: 0.696\n","testing loss: 0.493\n","##################################################\n","##########\tITERATION   111/1500\t##########\n","training loss: 0.753\n","testing loss: 0.469\n","##################################################\n","##########\tITERATION   112/1500\t##########\n","training loss: 0.735\n","testing loss: 0.504\n","##################################################\n","##########\tITERATION   113/1500\t##########\n","training loss: 0.696\n","testing loss: 0.477\n","##################################################\n","##########\tITERATION   114/1500\t##########\n","training loss: 0.632\n","testing loss: 0.514\n","##################################################\n","##########\tITERATION   115/1500\t##########\n","training loss: 0.692\n","testing loss: 0.514\n","##################################################\n","##########\tITERATION   116/1500\t##########\n","training loss: 0.658\n","testing loss: 0.578\n","##################################################\n","##########\tITERATION   117/1500\t##########\n","training loss: 0.647\n","testing loss: 0.469\n","##################################################\n","##########\tITERATION   118/1500\t##########\n","training loss: 0.739\n","testing loss: 0.451\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   119/1500\t##########\n","training loss: 0.672\n","testing loss: 0.506\n","##################################################\n","##########\tITERATION   120/1500\t##########\n","training loss: 0.669\n","testing loss: 0.467\n","##################################################\n","##########\tITERATION   121/1500\t##########\n","training loss: 0.707\n","testing loss: 0.469\n","##################################################\n","##########\tITERATION   122/1500\t##########\n","training loss: 0.717\n","testing loss: 0.502\n","##################################################\n","##########\tITERATION   123/1500\t##########\n","training loss: 0.734\n","testing loss: 0.51\n","##################################################\n","##########\tITERATION   124/1500\t##########\n","training loss: 0.641\n","testing loss: 0.533\n","##################################################\n","##########\tITERATION   125/1500\t##########\n","training loss: 0.677\n","testing loss: 0.514\n","##################################################\n","##########\tITERATION   126/1500\t##########\n","training loss: 0.634\n","testing loss: 0.49\n","##################################################\n","##########\tITERATION   127/1500\t##########\n","training loss: 0.712\n","testing loss: 0.476\n","##################################################\n","##########\tITERATION   128/1500\t##########\n","training loss: 0.683\n","testing loss: 0.528\n","##################################################\n","##########\tITERATION   129/1500\t##########\n","training loss: 0.656\n","testing loss: 0.488\n","##################################################\n","##########\tITERATION   130/1500\t##########\n","training loss: 0.643\n","testing loss: 0.484\n","##################################################\n","##########\tITERATION   131/1500\t##########\n","training loss: 0.658\n","testing loss: 0.551\n","##################################################\n","##########\tITERATION   132/1500\t##########\n","training loss: 0.672\n","testing loss: 0.531\n","##################################################\n","##########\tITERATION   133/1500\t##########\n","training loss: 0.736\n","testing loss: 0.458\n","##################################################\n","##########\tITERATION   134/1500\t##########\n","training loss: 0.683\n","testing loss: 0.459\n","##################################################\n","##########\tITERATION   135/1500\t##########\n","training loss: 0.696\n","testing loss: 0.508\n","##################################################\n","##########\tITERATION   136/1500\t##########\n","training loss: 0.666\n","testing loss: 0.441\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   137/1500\t##########\n","training loss: 0.691\n","testing loss: 0.452\n","##################################################\n","##########\tITERATION   138/1500\t##########\n","training loss: 0.735\n","testing loss: 0.458\n","##################################################\n","##########\tITERATION   139/1500\t##########\n","training loss: 0.694\n","testing loss: 0.47\n","##################################################\n","##########\tITERATION   140/1500\t##########\n","training loss: 0.711\n","testing loss: 0.494\n","##################################################\n","##########\tITERATION   141/1500\t##########\n","training loss: 0.629\n","testing loss: 0.502\n","##################################################\n","##########\tITERATION   142/1500\t##########\n","training loss: 0.65\n","testing loss: 0.438\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   143/1500\t##########\n","training loss: 0.695\n","testing loss: 0.469\n","##################################################\n","##########\tITERATION   144/1500\t##########\n","training loss: 0.691\n","testing loss: 0.48\n","##################################################\n","##########\tITERATION   145/1500\t##########\n","training loss: 0.631\n","testing loss: 0.533\n","##################################################\n","##########\tITERATION   146/1500\t##########\n","training loss: 0.632\n","testing loss: 0.487\n","##################################################\n","##########\tITERATION   147/1500\t##########\n","training loss: 0.801\n","testing loss: 0.506\n","##################################################\n","##########\tITERATION   148/1500\t##########\n","training loss: 0.664\n","testing loss: 0.508\n","##################################################\n","##########\tITERATION   149/1500\t##########\n","training loss: 0.671\n","testing loss: 0.445\n","##################################################\n","##########\tITERATION   150/1500\t##########\n","training loss: 0.704\n","testing loss: 0.516\n","##################################################\n","##########\tITERATION   151/1500\t##########\n","training loss: 0.693\n","testing loss: 0.501\n","##################################################\n","##########\tITERATION   152/1500\t##########\n","training loss: 0.68\n","testing loss: 0.507\n","##################################################\n","##########\tITERATION   153/1500\t##########\n","training loss: 0.664\n","testing loss: 0.469\n","##################################################\n","##########\tITERATION   154/1500\t##########\n","training loss: 0.736\n","testing loss: 0.463\n","##################################################\n","##########\tITERATION   155/1500\t##########\n","training loss: 0.619\n","testing loss: 0.49\n","##################################################\n","##########\tITERATION   156/1500\t##########\n","training loss: 0.621\n","testing loss: 0.505\n","##################################################\n","##########\tITERATION   157/1500\t##########\n","training loss: 0.607\n","testing loss: 0.569\n","##################################################\n","##########\tITERATION   158/1500\t##########\n","training loss: 0.686\n","testing loss: 0.479\n","##################################################\n","##########\tITERATION   159/1500\t##########\n","training loss: 0.675\n","testing loss: 0.489\n","##################################################\n","##########\tITERATION   160/1500\t##########\n","training loss: 0.661\n","testing loss: 0.483\n","##################################################\n","##########\tITERATION   161/1500\t##########\n","training loss: 0.768\n","testing loss: 0.495\n","##################################################\n","##########\tITERATION   162/1500\t##########\n","training loss: 0.687\n","testing loss: 0.474\n","##################################################\n","##########\tITERATION   163/1500\t##########\n","training loss: 0.619\n","testing loss: 0.5\n","##################################################\n","##########\tITERATION   164/1500\t##########\n","training loss: 0.573\n","testing loss: 0.541\n","##################################################\n","##########\tITERATION   165/1500\t##########\n","training loss: 0.648\n","testing loss: 0.544\n","##################################################\n","##########\tITERATION   166/1500\t##########\n","training loss: 0.62\n","testing loss: 0.439\n","##################################################\n","##########\tITERATION   167/1500\t##########\n","training loss: 0.678\n","testing loss: 0.426\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   168/1500\t##########\n","training loss: 0.716\n","testing loss: 0.457\n","##################################################\n","##########\tITERATION   169/1500\t##########\n","training loss: 0.67\n","testing loss: 0.499\n","##################################################\n","##########\tITERATION   170/1500\t##########\n","training loss: 0.69\n","testing loss: 0.514\n","##################################################\n","##########\tITERATION   171/1500\t##########\n","training loss: 0.672\n","testing loss: 0.496\n","##################################################\n","##########\tITERATION   172/1500\t##########\n","training loss: 0.609\n","testing loss: 0.494\n","##################################################\n","##########\tITERATION   173/1500\t##########\n","training loss: 0.591\n","testing loss: 0.479\n","##################################################\n","##########\tITERATION   174/1500\t##########\n","training loss: 0.621\n","testing loss: 0.491\n","##################################################\n","##########\tITERATION   175/1500\t##########\n","training loss: 0.643\n","testing loss: 0.435\n","##################################################\n","##########\tITERATION   176/1500\t##########\n","training loss: 0.66\n","testing loss: 0.463\n","##################################################\n","##########\tITERATION   177/1500\t##########\n","training loss: 0.594\n","testing loss: 0.498\n","##################################################\n","##########\tITERATION   178/1500\t##########\n","training loss: 0.622\n","testing loss: 0.459\n","##################################################\n","##########\tITERATION   179/1500\t##########\n","training loss: 0.63\n","testing loss: 0.519\n","##################################################\n","##########\tITERATION   180/1500\t##########\n","training loss: 0.589\n","testing loss: 0.496\n","##################################################\n","##########\tITERATION   181/1500\t##########\n","training loss: 0.647\n","testing loss: 0.44\n","##################################################\n","##########\tITERATION   182/1500\t##########\n","training loss: 0.664\n","testing loss: 0.506\n","##################################################\n","##########\tITERATION   183/1500\t##########\n","training loss: 0.669\n","testing loss: 0.504\n","##################################################\n","##########\tITERATION   184/1500\t##########\n","training loss: 0.678\n","testing loss: 0.543\n","##################################################\n","##########\tITERATION   185/1500\t##########\n","training loss: 0.606\n","testing loss: 0.48\n","##################################################\n","##########\tITERATION   186/1500\t##########\n","training loss: 0.658\n","testing loss: 0.523\n","##################################################\n","##########\tITERATION   187/1500\t##########\n","training loss: 0.611\n","testing loss: 0.529\n","##################################################\n","##########\tITERATION   188/1500\t##########\n","training loss: 0.677\n","testing loss: 0.472\n","##################################################\n","##########\tITERATION   189/1500\t##########\n","training loss: 0.721\n","testing loss: 0.49\n","##################################################\n","##########\tITERATION   190/1500\t##########\n","training loss: 0.597\n","testing loss: 0.459\n","##################################################\n","##########\tITERATION   191/1500\t##########\n","training loss: 0.683\n","testing loss: 0.458\n","##################################################\n","##########\tITERATION   192/1500\t##########\n","training loss: 0.623\n","testing loss: 0.531\n","##################################################\n","##########\tITERATION   193/1500\t##########\n","training loss: 0.641\n","testing loss: 0.536\n","##################################################\n","##########\tITERATION   194/1500\t##########\n","training loss: 0.656\n","testing loss: 0.534\n","##################################################\n","##########\tITERATION   195/1500\t##########\n","training loss: 0.571\n","testing loss: 0.457\n","##################################################\n","##########\tITERATION   196/1500\t##########\n","training loss: 0.561\n","testing loss: 0.436\n","##################################################\n","##########\tITERATION   197/1500\t##########\n","training loss: 0.612\n","testing loss: 0.467\n","##################################################\n","##########\tITERATION   198/1500\t##########\n","training loss: 0.629\n","testing loss: 0.456\n","##################################################\n","##########\tITERATION   199/1500\t##########\n","training loss: 0.615\n","testing loss: 0.453\n","##################################################\n","##########\tITERATION   200/1500\t##########\n","training loss: 0.638\n","testing loss: 0.47\n","##################################################\n","##########\tITERATION   201/1500\t##########\n","training loss: 0.711\n","testing loss: 0.483\n","##################################################\n","##########\tITERATION   202/1500\t##########\n","training loss: 0.598\n","testing loss: 0.494\n","##################################################\n","##########\tITERATION   203/1500\t##########\n","training loss: 0.58\n","testing loss: 0.508\n","##################################################\n","##########\tITERATION   204/1500\t##########\n","training loss: 0.625\n","testing loss: 0.467\n","##################################################\n","##########\tITERATION   205/1500\t##########\n","training loss: 0.637\n","testing loss: 0.469\n","##################################################\n","##########\tITERATION   206/1500\t##########\n","training loss: 0.714\n","testing loss: 0.481\n","##################################################\n","##########\tITERATION   207/1500\t##########\n","training loss: 0.608\n","testing loss: 0.465\n","##################################################\n","##########\tITERATION   208/1500\t##########\n","training loss: 0.673\n","testing loss: 0.465\n","##################################################\n","##########\tITERATION   209/1500\t##########\n","training loss: 0.648\n","testing loss: 0.453\n","##################################################\n","##########\tITERATION   210/1500\t##########\n","training loss: 0.602\n","testing loss: 0.483\n","##################################################\n","##########\tITERATION   211/1500\t##########\n","training loss: 0.665\n","testing loss: 0.473\n","##################################################\n","##########\tITERATION   212/1500\t##########\n","training loss: 0.595\n","testing loss: 0.472\n","(<class 'Exception'>, Exception('Net stop to learn'), <traceback object at 0x7fce9007ed40>)\n","best testing loss: 0.426 @ 167\n","##########\tEND\t##########\n","fridge 0.001\n","fixed mode\n","working on: cuda:0\n","##########\tITERATION   0/1500\t##########\n","training loss: 0.987\n","testing loss: 0.698\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   1/1500\t##########\n","training loss: 1.091\n","testing loss: 0.725\n","##################################################\n","##########\tITERATION   2/1500\t##########\n","training loss: 1.03\n","testing loss: 0.731\n","##################################################\n","##########\tITERATION   3/1500\t##########\n","training loss: 0.965\n","testing loss: 0.753\n","##################################################\n","##########\tITERATION   4/1500\t##########\n","training loss: 0.942\n","testing loss: 0.664\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   5/1500\t##########\n","training loss: 0.977\n","testing loss: 0.741\n","##################################################\n","##########\tITERATION   6/1500\t##########\n","training loss: 0.994\n","testing loss: 0.763\n","##################################################\n","##########\tITERATION   7/1500\t##########\n","training loss: 0.895\n","testing loss: 0.739\n","##################################################\n","##########\tITERATION   8/1500\t##########\n","training loss: 0.938\n","testing loss: 0.675\n","##################################################\n","##########\tITERATION   9/1500\t##########\n","training loss: 0.925\n","testing loss: 0.65\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   10/1500\t##########\n","training loss: 0.924\n","testing loss: 0.654\n","##################################################\n","##########\tITERATION   11/1500\t##########\n","training loss: 0.906\n","testing loss: 0.719\n","##################################################\n","##########\tITERATION   12/1500\t##########\n","training loss: 0.899\n","testing loss: 0.703\n","##################################################\n","##########\tITERATION   13/1500\t##########\n","training loss: 0.832\n","testing loss: 0.694\n","##################################################\n","##########\tITERATION   14/1500\t##########\n","training loss: 0.857\n","testing loss: 0.651\n","##################################################\n","##########\tITERATION   15/1500\t##########\n","training loss: 0.828\n","testing loss: 0.623\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   16/1500\t##########\n","training loss: 0.814\n","testing loss: 0.649\n","##################################################\n","##########\tITERATION   17/1500\t##########\n","training loss: 0.83\n","testing loss: 0.646\n","##################################################\n","##########\tITERATION   18/1500\t##########\n","training loss: 0.895\n","testing loss: 0.537\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   19/1500\t##########\n","training loss: 0.763\n","testing loss: 0.565\n","##################################################\n","##########\tITERATION   20/1500\t##########\n","training loss: 0.874\n","testing loss: 0.715\n","##################################################\n","##########\tITERATION   21/1500\t##########\n","training loss: 0.839\n","testing loss: 0.601\n","##################################################\n","##########\tITERATION   22/1500\t##########\n","training loss: 0.773\n","testing loss: 0.561\n","##################################################\n","##########\tITERATION   23/1500\t##########\n","training loss: 0.723\n","testing loss: 0.566\n","##################################################\n","##########\tITERATION   24/1500\t##########\n","training loss: 0.845\n","testing loss: 0.715\n","##################################################\n","##########\tITERATION   25/1500\t##########\n","training loss: 0.888\n","testing loss: 0.597\n","##################################################\n","##########\tITERATION   26/1500\t##########\n","training loss: 0.85\n","testing loss: 0.603\n","##################################################\n","##########\tITERATION   27/1500\t##########\n","training loss: 0.98\n","testing loss: 0.554\n","##################################################\n","##########\tITERATION   28/1500\t##########\n","training loss: 0.781\n","testing loss: 0.575\n","##################################################\n","##########\tITERATION   29/1500\t##########\n","training loss: 0.84\n","testing loss: 0.59\n","##################################################\n","##########\tITERATION   30/1500\t##########\n","training loss: 0.859\n","testing loss: 0.644\n","##################################################\n","##########\tITERATION   31/1500\t##########\n","training loss: 0.828\n","testing loss: 0.576\n","##################################################\n","##########\tITERATION   32/1500\t##########\n","training loss: 0.785\n","testing loss: 0.552\n","##################################################\n","##########\tITERATION   33/1500\t##########\n","training loss: 0.82\n","testing loss: 0.551\n","##################################################\n","##########\tITERATION   34/1500\t##########\n","training loss: 0.833\n","testing loss: 0.635\n","##################################################\n","##########\tITERATION   35/1500\t##########\n","training loss: 0.838\n","testing loss: 0.627\n","##################################################\n","##########\tITERATION   36/1500\t##########\n","training loss: 0.847\n","testing loss: 0.596\n","##################################################\n","##########\tITERATION   37/1500\t##########\n","training loss: 0.786\n","testing loss: 0.616\n","##################################################\n","##########\tITERATION   38/1500\t##########\n","training loss: 0.836\n","testing loss: 0.555\n","##################################################\n","##########\tITERATION   39/1500\t##########\n","training loss: 0.816\n","testing loss: 0.546\n","##################################################\n","##########\tITERATION   40/1500\t##########\n","training loss: 0.846\n","testing loss: 0.528\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   41/1500\t##########\n","training loss: 0.811\n","testing loss: 0.543\n","##################################################\n","##########\tITERATION   42/1500\t##########\n","training loss: 0.79\n","testing loss: 0.635\n","##################################################\n","##########\tITERATION   43/1500\t##########\n","training loss: 0.737\n","testing loss: 0.598\n","##################################################\n","##########\tITERATION   44/1500\t##########\n","training loss: 0.817\n","testing loss: 0.534\n","##################################################\n","##########\tITERATION   45/1500\t##########\n","training loss: 0.827\n","testing loss: 0.553\n","##################################################\n","##########\tITERATION   46/1500\t##########\n","training loss: 0.762\n","testing loss: 0.563\n","##################################################\n","##########\tITERATION   47/1500\t##########\n","training loss: 0.881\n","testing loss: 0.594\n","##################################################\n","##########\tITERATION   48/1500\t##########\n","training loss: 0.863\n","testing loss: 0.579\n","##################################################\n","##########\tITERATION   49/1500\t##########\n","training loss: 0.779\n","testing loss: 0.513\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   50/1500\t##########\n","training loss: 0.777\n","testing loss: 0.57\n","##################################################\n","##########\tITERATION   51/1500\t##########\n","training loss: 0.801\n","testing loss: 0.572\n","##################################################\n","##########\tITERATION   52/1500\t##########\n","training loss: 0.802\n","testing loss: 0.548\n","##################################################\n","##########\tITERATION   53/1500\t##########\n","training loss: 0.784\n","testing loss: 0.569\n","##################################################\n","##########\tITERATION   54/1500\t##########\n","training loss: 0.841\n","testing loss: 0.581\n","##################################################\n","##########\tITERATION   55/1500\t##########\n","training loss: 0.817\n","testing loss: 0.575\n","##################################################\n","##########\tITERATION   56/1500\t##########\n","training loss: 0.792\n","testing loss: 0.635\n","##################################################\n","##########\tITERATION   57/1500\t##########\n","training loss: 0.749\n","testing loss: 0.57\n","##################################################\n","##########\tITERATION   58/1500\t##########\n","training loss: 0.734\n","testing loss: 0.539\n","##################################################\n","##########\tITERATION   59/1500\t##########\n","training loss: 0.783\n","testing loss: 0.554\n","##################################################\n","##########\tITERATION   60/1500\t##########\n","training loss: 0.837\n","testing loss: 0.617\n","##################################################\n","##########\tITERATION   61/1500\t##########\n","training loss: 0.831\n","testing loss: 0.643\n","##################################################\n","##########\tITERATION   62/1500\t##########\n","training loss: 0.798\n","testing loss: 0.632\n","##################################################\n","##########\tITERATION   63/1500\t##########\n","training loss: 0.712\n","testing loss: 0.533\n","##################################################\n","##########\tITERATION   64/1500\t##########\n","training loss: 0.851\n","testing loss: 0.576\n","##################################################\n","##########\tITERATION   65/1500\t##########\n","training loss: 0.816\n","testing loss: 0.566\n","##################################################\n","##########\tITERATION   66/1500\t##########\n","training loss: 0.812\n","testing loss: 0.552\n","##################################################\n","##########\tITERATION   67/1500\t##########\n","training loss: 0.786\n","testing loss: 0.533\n","##################################################\n","##########\tITERATION   68/1500\t##########\n","training loss: 0.771\n","testing loss: 0.564\n","##################################################\n","##########\tITERATION   69/1500\t##########\n","training loss: 0.768\n","testing loss: 0.53\n","##################################################\n","##########\tITERATION   70/1500\t##########\n","training loss: 0.768\n","testing loss: 0.559\n","##################################################\n","##########\tITERATION   71/1500\t##########\n","training loss: 0.81\n","testing loss: 0.532\n","##################################################\n","##########\tITERATION   72/1500\t##########\n","training loss: 0.811\n","testing loss: 0.507\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   73/1500\t##########\n","training loss: 0.824\n","testing loss: 0.58\n","##################################################\n","##########\tITERATION   74/1500\t##########\n","training loss: 0.746\n","testing loss: 0.6\n","##################################################\n","##########\tITERATION   75/1500\t##########\n","training loss: 0.748\n","testing loss: 0.469\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   76/1500\t##########\n","training loss: 0.813\n","testing loss: 0.483\n","##################################################\n","##########\tITERATION   77/1500\t##########\n","training loss: 0.853\n","testing loss: 0.513\n","##################################################\n","##########\tITERATION   78/1500\t##########\n","training loss: 0.764\n","testing loss: 0.598\n","##################################################\n","##########\tITERATION   79/1500\t##########\n","training loss: 0.749\n","testing loss: 0.548\n","##################################################\n","##########\tITERATION   80/1500\t##########\n","training loss: 0.755\n","testing loss: 0.539\n","##################################################\n","##########\tITERATION   81/1500\t##########\n","training loss: 0.865\n","testing loss: 0.504\n","##################################################\n","##########\tITERATION   82/1500\t##########\n","training loss: 0.85\n","testing loss: 0.489\n","##################################################\n","##########\tITERATION   83/1500\t##########\n","training loss: 0.725\n","testing loss: 0.555\n","##################################################\n","##########\tITERATION   84/1500\t##########\n","training loss: 0.755\n","testing loss: 0.576\n","##################################################\n","##########\tITERATION   85/1500\t##########\n","training loss: 0.737\n","testing loss: 0.522\n","##################################################\n","##########\tITERATION   86/1500\t##########\n","training loss: 0.772\n","testing loss: 0.512\n","##################################################\n","##########\tITERATION   87/1500\t##########\n","training loss: 0.789\n","testing loss: 0.462\n","best testing loss, saving network\n","##################################################\n","##########\tITERATION   88/1500\t##########\n","training loss: 0.741\n","testing loss: 0.515\n","##################################################\n","##########\tITERATION   89/1500\t##########\n","training loss: 0.792\n","testing loss: 0.518\n","##################################################\n","##########\tITERATION   90/1500\t##########\n","training loss: 0.791\n","testing loss: 0.472\n","##################################################\n","##########\tITERATION   91/1500\t##########\n","training loss: 0.761\n","testing loss: 0.525\n","##################################################\n","##########\tITERATION   92/1500\t##########\n","training loss: 0.81\n","testing loss: 0.531\n","##################################################\n","##########\tITERATION   93/1500\t##########\n","training loss: 0.789\n","testing loss: 0.636\n","##################################################\n","##########\tITERATION   94/1500\t##########\n","training loss: 0.758\n","testing loss: 0.568\n","##################################################\n","##########\tITERATION   95/1500\t##########\n","training loss: 0.693\n","testing loss: 0.54\n","##################################################\n","##########\tITERATION   96/1500\t##########\n","training loss: 0.761\n","testing loss: 0.516\n","##################################################\n","##########\tITERATION   97/1500\t##########\n","training loss: 0.747\n","testing loss: 0.528\n","##################################################\n","##########\tITERATION   98/1500\t##########\n","training loss: 0.803\n","testing loss: 0.572\n","##################################################\n","##########\tITERATION   99/1500\t##########\n","training loss: 0.679\n","testing loss: 0.622\n","##################################################\n","##########\tITERATION   100/1500\t##########\n","training loss: 0.744\n","testing loss: 0.498\n","##################################################\n","##########\tITERATION   101/1500\t##########\n","training loss: 0.716\n","testing loss: 0.505\n","##################################################\n","##########\tITERATION   102/1500\t##########\n","training loss: 0.818\n","testing loss: 0.535\n","##################################################\n","##########\tITERATION   103/1500\t##########\n","training loss: 0.695\n","testing loss: 0.563\n","##################################################\n","##########\tITERATION   104/1500\t##########\n","training loss: 0.815\n","testing loss: 0.522\n","##################################################\n","##########\tITERATION   105/1500\t##########\n","training loss: 0.778\n","testing loss: 0.505\n","##################################################\n","##########\tITERATION   106/1500\t##########\n","training loss: 0.706\n","testing loss: 0.53\n","##################################################\n","##########\tITERATION   107/1500\t##########\n","training loss: 0.783\n","testing loss: 0.628\n","##################################################\n","##########\tITERATION   108/1500\t##########\n","training loss: 0.697\n","testing loss: 0.588\n","##################################################\n","##########\tITERATION   109/1500\t##########\n","training loss: 0.742\n","testing loss: 0.532\n","##################################################\n","##########\tITERATION   110/1500\t##########\n","training loss: 0.7\n","testing loss: 0.49\n","##################################################\n","##########\tITERATION   111/1500\t##########\n","training loss: 0.712\n","testing loss: 0.531\n","##################################################\n","##########\tITERATION   112/1500\t##########\n","training loss: 0.755\n","testing loss: 0.601\n","##################################################\n","##########\tITERATION   113/1500\t##########\n","training loss: 0.786\n","testing loss: 0.607\n","##################################################\n","##########\tITERATION   114/1500\t##########\n","training loss: 0.737\n","testing loss: 0.552\n","##################################################\n","##########\tITERATION   115/1500\t##########\n","training loss: 0.758\n","testing loss: 0.529\n","##################################################\n","##########\tITERATION   116/1500\t##########\n","training loss: 0.69\n","testing loss: 0.523\n","##################################################\n","##########\tITERATION   117/1500\t##########\n","training loss: 0.794\n","testing loss: 0.512\n","(<class 'Exception'>, Exception('Net stop to learn'), <traceback object at 0x7fce97730880>)\n","best testing loss: 0.462 @ 87\n","##########\tEND\t##########\n","saved plot\n","saved metrics\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","import os\n","import sys\n","sys.path.append('./gdrive/MyDrive/università/progetti/digital adaptive/digital adaptive/codice')\n","os.chdir('./gdrive/MyDrive/università/progetti/digital adaptive/digital adaptive/codice/')\n","from mainloop import*#PER CAMBIARE STANDARDIZZAZIONE CAMBIARE COMMENTI SU MAINLOOP\n","elecs=['dishwasher','fridge','kettle','microwave','washing machine']\n","elecs=['fridge']\n","path='./'#dentro questa cartella devono esserci quelle con i nomi degli elettrodomestici VUOTE dove saranno salvati i dati\n","for elec in elecs:\n","  parPathTlnNoAd=path+elec+'/'+elec+' TLN no Ad'\n","  parPathTlnAd=path+elec+'/'+elec+' TLN Ad'\n","  parPathAlexnet=path+elec+'/'+elec+' Baseline'\n","  #train TLN no ad\n","  adapt=False\n","  net=TLN()\n","  net=net.float()\n","  net=net.to(device)\n","  net=net.train()\n","  trainNet(elec,net,parPathTlnNoAd,adapt,1,30)\n","  #train TLN ad\n","  adapt=True\n","  net=TLN()\n","  #net.load_state_dict(torch.load(parPathTlnNoAd))\n","  net=net.float()\n","  net=net.to(device)\n","  net=net.train()\n","  trainNet(elec,net,parPathTlnAd,adapt,1,45)\n","  #train Baseline\n","  adapt=False\n","  net=Alexnet()\n","  net=net.float()\n","  net=net.to(device)\n","  net=net.train()\n","  trainNet(elec,net,parPathAlexnet,adapt,1,30)\n","  #comparazione\n","  netTlnNoAd=TLN()\n","  netTlnNoAd.load_state_dict(torch.load(parPathTlnNoAd))\n","  netTlnNoAd=netTlnNoAd.float()\n","  netTlnNoAd=netTlnNoAd.eval()\n","  netTlnNoAd=netTlnNoAd.to(device)\n","  #\n","  netTlnAd=TLN()\n","  netTlnAd.load_state_dict(torch.load(parPathTlnAd))\n","  netTlnAd=netTlnAd.float()\n","  netTlnAd=netTlnAd.eval()\n","  netTlnAd=netTlnAd.to(device)\n","  #\n","  netAlexnet=Alexnet()\n","  netAlexnet.load_state_dict(torch.load(parPathAlexnet))\n","  netAlexnet=netAlexnet.float()\n","  netAlexnet=netAlexnet.eval()\n","  netAlexnet=netAlexnet.to(device)\n","  #\n","  compareNet(elec,[netAlexnet,netTlnNoAd,netTlnAd],['Baseline','TLN no Ad','TLN Ad'],path+elec+'/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16541,"status":"ok","timestamp":1687455140857,"user":{"displayName":"Andrea Papiri","userId":"03585521923946040254"},"user_tz":-120},"id":"s60T0cxENQoZ","outputId":"64220280-8992-46ee-d400-53e0fae4afda"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","import os\n","import sys\n","sys.path.append('./gdrive/MyDrive/università/progetti/digital adaptive/digital adaptive/codice')\n","os.chdir('./gdrive/MyDrive/università/progetti/digital adaptive/digital adaptive/codice/')\n","from utility import*\n","#elecs=['dish washer','fridge','kettle','microwave','washing machine']\n","elec='washing machine'\n","xTraining,yTraining,xValidation,yValidation,xTesting,yTesting=createSet(\n","        ['./data/dataset/5_'+elec+'_x.npy','./data/dataset/1_'+elec+'_x.npy'],\n","        ['./data/dataset/5_'+elec+'_y.npy','./data/dataset/1_'+elec+'_y.npy'],\n","        ['./data/dataset/2_'+elec+'_x.npy'],\n","        ['./data/dataset/2_'+elec+'_y.npy'])\n","aggregateTr,groundTruthTr=batchShuffle(xTraining,yTraining,batchSize=1)\n","aggregateTe,groundTruthTe=batchShuffle(xTesting,yTesting,batchSize=1)\n","overlapPlot([groundTruthTe],['ground truth washing machine',],'./','')\n","#torch.cuda.empty_cache()\n","#groundTruthTr=torch.rand([256,32,600],device=device,requires_grad=True)\n","#groundTruthTe=torch.rand([256,32,600],device=device,requires_grad=True)\n","#print(mmd(a,b))\n","#print(groundTruthTr.shape)\n","#print(coral(groundTruthTe.flatten(1),aggregateTr.flatten(1)))\n","#print(mmd(groundTruthTe.flatten(1),aggregateTr.flatten(1)))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO1yOQEtpgcS5l7iOlEr24P"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}